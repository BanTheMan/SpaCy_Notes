{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7043dac5",
   "metadata": {},
   "source": [
    "**Important:**\n",
    "When running a cell, run the section in sequential order to ensure all prerequisites are met for that cell.\n",
    "\n",
    "**Setup**\n",
    "\n",
    "Install spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dbb07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download spaCy\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "474a38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, make sure spacy is up to date\n",
    "!pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a438fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89effffc",
   "metadata": {},
   "source": [
    "__Install and load a spaCy model__\n",
    "\n",
    "spaCy models are pretrained statistical models that allow you to perform necessary NLP functions.\n",
    "\n",
    "In this case, we will be using the 'en_core_web_md' as it has more features than the default ('en_core_web_sm'). \n",
    "(see https://spacy.io/models/en/)\n",
    "Each model contains a preset pipeline to process text through. (You will learn about pipelines further down)\n",
    "\n",
    "Sample info:\n",
    "'en' means english\n",
    "'sm' means small\n",
    "'md' means medium\n",
    "'lg' means large\n",
    "size of model reflects its accuracy in its performance of NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c34dd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8800f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model as a natural language processer\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131cb62e",
   "metadata": {},
   "source": [
    "**Visualization**\n",
    "\n",
    "Some sections will provide an example visualization.\n",
    "\n",
    "For more information on visualization including more visualizer functions, parameters, available arguments and their output, consult this documentation:\n",
    "https://spacy.io/api/top-level#displacy_options\n",
    "Under 'displaCy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa4f88",
   "metadata": {},
   "source": [
    "**SpaCy Data Structures**\n",
    "1. Doc\n",
    "2. Token\n",
    "3. Vocabulary\n",
    "4. Lexeme\n",
    "5. Span\n",
    "6. Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc5758",
   "metadata": {},
   "source": [
    "**1. The 'Doc'**\n",
    "\n",
    "A 'Doc' represents a piece of text. It is a sequence of Token objects that contain information about the text it represents. Docs are the primary data structure for text processing in spaCy.\n",
    "    \n",
    "All doc methods and attributes:\n",
    "https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20989fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac2a6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will become a doc!\n"
     ]
    }
   ],
   "source": [
    "# Create a 'Doc'\n",
    "\n",
    "text = \"I will become a doc!\"\n",
    "\n",
    "doc = nlp(text)\n",
    "# a doc is a list of token objects\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb1b37b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0: I\n",
      "index 1: will\n",
      "index 2: become\n",
      "index 3: a\n",
      "index 4: doc\n",
      "index 5: !\n"
     ]
    }
   ],
   "source": [
    "# Accessing tokens inside of Doc\n",
    "\n",
    "# Iterate through doc tokens\n",
    "index = 0\n",
    "while index < len(doc):\n",
    "    token_at_index = doc[index]\n",
    "    print(f\"index {index}: {token_at_index}\")\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f73a34df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"567e8bec2b244336b24f0957aaefcb74-0\" class=\"displacy\" width=\"925\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">will</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">become</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">doc!</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-567e8bec2b244336b24f0957aaefcb74-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-567e8bec2b244336b24f0957aaefcb74-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-567e8bec2b244336b24f0957aaefcb74-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-567e8bec2b244336b24f0957aaefcb74-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-567e8bec2b244336b24f0957aaefcb74-0-2\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-567e8bec2b244336b24f0957aaefcb74-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-567e8bec2b244336b24f0957aaefcb74-0-3\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-567e8bec2b244336b24f0957aaefcb74-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,179.0 L758.0,167.0 742.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example for Visualizing a Doc\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "# this visualization represents each word's dependencies (covered later)\n",
    "# for more information, go up to the 'Visualization' cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d2daa9",
   "metadata": {},
   "source": [
    "**2. The 'Token'**\n",
    "\n",
    "A Token can be a single word, punctuation mark, or other linguistic unit that is stored within a 'Doc'. Each token has properties of various linguistic annotations including part-of-speech tags, named entity labels, and syntactic dependencies.\n",
    "    \n",
    "<u>pos tags:<u>\n",
    "* Abbreviated representations of a part of speech based on the word's definition and context.\n",
    "\n",
    "<u>named entity:<u> \n",
    "* A real-world object that is assigned a name. (NER is covered later)\n",
    "\n",
    "<u>syntactic dependencies:<u> \n",
    "    \n",
    "* The relations between individual words of a sentence.\n",
    "\n",
    "All token methods and attributes:\n",
    "https://spacy.io/api/token#vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc906a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "# Create Doc\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "text = \"These will become tokens\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9daed569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first token: These\n",
      "These\n",
      "will\n",
      "become\n",
      "tokens\n"
     ]
    }
   ],
   "source": [
    "# Accessing and interacting with tokens\n",
    "\n",
    "# Indexing\n",
    "first_token = doc[0]\n",
    "print(f\"first token: {first_token}\")\n",
    "# Iterating\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "148c628c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: These\n",
      "\n",
      "Base form: these\n",
      "\n",
      "Part of speech tag: PRON\n",
      "\n",
      "Detailed pos tag: DT\n",
      "\n",
      "Dependency label: nsubj\n",
      "\n",
      "Word shape: Xxxxx\n",
      "\n",
      "\n",
      "\n",
      "Is alpha character: True\n",
      "\n",
      "Is stop word: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing and interacting with tokens\n",
    "\n",
    "# Token Attributes\n",
    "\"\"\"\n",
    "Many of a token's attributes are hashes\n",
    "but can be retrieved as their string from\n",
    "by placing an underscore at the end\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "Original text: {first_token.text}\\n\n",
    "Base form: {first_token.lemma_}\\n\n",
    "Part of speech tag: {first_token.pos_}\\n\n",
    "Detailed pos tag: {first_token.tag_}\\n\n",
    "Dependency label: {first_token.dep_}\\n\n",
    "Word shape: {first_token.shape_}\\n\n",
    "\"\"\"\n",
    ")\n",
    "    \n",
    "# Token Methods \n",
    "\n",
    "# alpha characters are chars from the alphabet (non-numerical)\n",
    "# stop words are words with little meaning (covered later)\n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "Is alpha character: {first_token.is_alpha}\\n\n",
    "Is stop word: {first_token.is_stop}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d8b1f",
   "metadata": {},
   "source": [
    "**3. Vocabulary**\n",
    "\n",
    "A storage class; a collection of all unique words or tokens used by spaCy in a particular language model. Vocabulary includes word vectors (covered later) and integer IDs associated with each word. Great for text processing and memory.\n",
    "    \n",
    "Vocabularies customization will not be covered\n",
    "\n",
    "For all Vocab methods and attributes:\n",
    "https://spacy.io/api/vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a1ca4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites \n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b57be408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count: 764\n",
      "\n",
      "apple is in vocab: False\n",
      "\n",
      "word ID: 8566208034543834098\n",
      "\n",
      "apple has vector: True\n",
      "\n",
      "word vector for 'apple': [-1.0084  -2.0308  -0.64185  2.6928   0.31771 -2.6662  -3.7372   5.4714 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing the 'Vocabulary'\n",
    "\n",
    "vocab = nlp.vocab\n",
    "\n",
    "\n",
    "# Accessing Vocabulary Attributes\n",
    "\n",
    "word_count = len(vocab)\n",
    "# Returns the total count of unique words in the vocabulary\n",
    "print(f\"word count: {word_count}\\n\")\n",
    "\n",
    "\n",
    "word = \"apple\"\n",
    "\n",
    "# Check if the word is in the vocabulary\n",
    "is_in_vocab = word in vocab\n",
    "print(f\"{word} is in vocab: {is_in_vocab}\\n\")\n",
    "\n",
    "\n",
    "# Find an ID\n",
    "word_id = vocab[word].orth\n",
    "# words have IDs inside the vocabulary\n",
    "# the ID can be used to find the word in the vocab\n",
    "print(f\"word ID: {word_id}\\n\")\n",
    "\n",
    "\n",
    "# Check if word has a vector\n",
    "has_vec = vocab.has_vector(word_id)\n",
    "# Returns True or False reflecting if the provided word has a vector in the vocab\n",
    "# NOTE: word and word_id are interchangeable when finding its attributes\n",
    "print(f\"{word} has vector: {has_vec}\\n\")\n",
    "\n",
    "\n",
    "# Retrieve word vector\n",
    "word_vector = vocab[word_id].vector\n",
    "# Returns a pre-trained word vector (list of numbers) that represent the provided word's meaning\n",
    "print(f\"word vector for '{word}': {word_vector[0:8]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38606696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab section:\n",
      "nuthin\n",
      "ü.\n",
      "p.m\n",
      "Kan\n",
      "Mar\n",
      "When's\n",
      " \n",
      "Sept.\n",
      "c.\n",
      "Mont.\n"
     ]
    }
   ],
   "source": [
    "# Iterating through a vocab\n",
    "\n",
    "print(\"Vocab section:\")\n",
    "Vocab_section = list(nlp.vocab)[:10]\n",
    "# Returns a list of lexemes in the vocab up to index 10\n",
    "\n",
    "for lexeme in Vocab_section:\n",
    "    word = lexeme.text\n",
    "    # Returns the string version of the lexeme (covered later)\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c8345f",
   "metadata": {},
   "source": [
    "**4. The 'Lexeme'**\n",
    "\n",
    "A basic unit of a word in spaCy's vocabulary. It includes information about the word's string representation, orthographic features, and linguistic attributes. Can be used to efficiently access word information from the vocabulary.\n",
    "    \n",
    "For all Lexeme methods and attributes:\n",
    "https://spacy.io/api/lexeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e4865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Setup\n",
    "text = \"My word is Paper\"\n",
    "doc = nlp(text)\n",
    "vocab = nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "430ed314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Way #1:\n",
      "\n",
      "my word: Paper\n",
      "my lexeme: <spacy.lexeme.Lexeme object at 0x7f0c7955f100>\n",
      "\n",
      "Way #2:\n",
      "\n",
      "my word: Paper\n",
      "my lexeme: <spacy.lexeme.Lexeme object at 0x7f0b47f77e40>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing a lexeme\n",
    "\n",
    "# Way #1: use a doc\n",
    "# Will be more accurate\n",
    "\n",
    "print(\"Way #1:\\n\")\n",
    "\n",
    "my_word = doc[len(doc)-1]\n",
    "print(f\"my word: {my_word}\")\n",
    "\n",
    "my_lexeme = my_word.lex\n",
    "print(f\"my lexeme: {my_lexeme}\\n\")\n",
    "\n",
    "\n",
    "# Way #2: use a vocabulary\n",
    "\n",
    "print(\"Way #2:\")\n",
    "\n",
    "word = \"Paper\"\n",
    "lexeme = vocab[word]\n",
    "\n",
    "print(f\"\"\"\n",
    "my word: {word}\n",
    "my lexeme: {lexeme}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40ca0dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexeme word: Paper\n",
      "\n",
      "The lexeme is:\n",
      "lower: False\n",
      "upper: False\n",
      "title: True\n",
      "stop word: False\n",
      "currency: False\n",
      "\n",
      "Other Properties:\n",
      "from language: en\n",
      "shape: Xxxxx\n",
      "sentiment: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing lexeme attributes\n",
    "\n",
    "# Text\n",
    "\n",
    "lexeme_text = my_lexeme.text\n",
    "# Returns the text representation of the lexeme\n",
    "print(f\"lexeme word: {lexeme_text}\")\n",
    "\n",
    "\n",
    "# Orthographic features\n",
    "\n",
    "is_lower = my_lexeme.is_lower\n",
    "# Lowercase\n",
    "is_upper = my_lexeme.is_upper\n",
    "# Uppercase\n",
    "is_title = my_lexeme.is_title\n",
    "# Titlecase\n",
    "is_stop = my_lexeme.is_stop\n",
    "# Stop word \n",
    "# (little contextual meaning)\n",
    "is_currency = my_lexeme.is_currency\n",
    "# Currency of some kind\n",
    "lang = my_lexeme.lang_\n",
    "# Language of parent vocab\n",
    "shape = my_lexeme.shape_\n",
    "# Shape of word (case, length, etc.)\n",
    "sentiment = my_lexeme.sentiment\n",
    "# point value that represents emotional sentiment of word\n",
    "# (covered later)\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "The lexeme is:\n",
    "lower: {is_lower}\n",
    "upper: {is_upper}\n",
    "title: {is_title}\n",
    "stop word: {is_stop}\n",
    "currency: {is_currency}\n",
    "\n",
    "Other Properties:\n",
    "from language: {lang}\n",
    "shape: {shape}\n",
    "sentiment: {sentiment}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c27aa09",
   "metadata": {},
   "source": [
    "**5. the 'Span'**\n",
    "\n",
    "A fragment of text which belongs to some category. It is stored as a contiguous sequence of tokens within a Doc; allowing you to work with a subset of tokens in a document.\n",
    "    \n",
    "For all span methods and attributes:\n",
    "https://spacy.io/api/span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebc58328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token #0: I\n",
      "token #1: am\n",
      "token #2: a\n",
      "token #3: doc\n",
      "token #4: ,\n",
      "token #5: but\n",
      "token #6: here\n",
      "token #7: is\n",
      "token #8: a\n",
      "token #9: span\n",
      "token #10: .\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Setup\n",
    "text = \"I am a doc, but here is a span.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for i, token in enumerate(doc):\n",
    "    print(f\"token #{i}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1df3df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token #6: here\n",
      "token #7: is\n",
      "token #8: a\n",
      "token #9: span\n"
     ]
    }
   ],
   "source": [
    "# Create a span\n",
    "\n",
    "# Select tokens in a doc\n",
    "span = doc[6:10]\n",
    "# Creates a span that includes tokens 5, 6, 7, 8, and 9\n",
    "# You can access the attributes of each token\n",
    "\n",
    "for i, token in enumerate(span, start=6):\n",
    "    print(f\"token #{i}: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f33e07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This sentence is a span.\n"
     ]
    }
   ],
   "source": [
    "# Create a span of a sentence\n",
    "\n",
    "# Setup\n",
    "text = \"I don't want this sentence. This sentence is a span. Not this one.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "sents = []\n",
    "# iterate through the sentences in a doc\n",
    "for sentence in doc.sents:\n",
    "    # .sents returns an object\n",
    "    sents.append(sentence)\n",
    "    # add each sentence to a list\n",
    "\n",
    "span = sents[1]\n",
    "# select which sentence to become a span\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9edba",
   "metadata": {},
   "source": [
    "<u>Span Use Cases:<u>\n",
    "\n",
    "- Custom NER (covered later):\n",
    "    - Can use a span to define your own named entities and use matching (covered later) to analyze text\n",
    "- Text Processing (covered later)\n",
    "    - Allows you to process and analyze a specific span of text\n",
    "- Text Classification (covered later)\n",
    "    - Can use spans to represent points of interest throughout a document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43976da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span:\n",
      "here is a span \n",
      "\n",
      "Index 0: h\n",
      "Index 1: e\n",
      "Index 2: r\n",
      "Index 3: e\n",
      "Index 4:  \n",
      "Index 5: i\n",
      "Index 6: s\n",
      "Index 7:  \n",
      "Index 8: a\n",
      "Index 9:  \n",
      "Index 10: s\n",
      "Index 11: p\n",
      "Index 12: a\n",
      "Index 13: n\n",
      "Index 14:  \n",
      "\n",
      "Character Span from 8-14:\n",
      "a span\n"
     ]
    }
   ],
   "source": [
    "# Span Method example\n",
    "\n",
    "text = \"I am a doc, but here is a span.\"\n",
    "doc = nlp(text)\n",
    "span = doc[6:10]\n",
    "\n",
    "\n",
    "# Span index visualization\n",
    "\n",
    "Span_string = \"\"\n",
    "for token in span:\n",
    "    Span_string += token.text + \" \"\n",
    "print(f\"Span:\\n{Span_string}\\n\")\n",
    "\n",
    "for i, char in enumerate(Span_string):\n",
    "      print(f\"Index {i}: {char}\")\n",
    "        \n",
    "\n",
    "# Character span method\n",
    "\n",
    "start_index = 8\n",
    "end_index = 14\n",
    "character_span = span.char_span(start_index, end_index)\n",
    "# Returns a span of tokens from the specified slice from the original span\n",
    "\n",
    "print(f\"\\nCharacter Span from {start_index}-{end_index}:\\n{character_span}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36dbe17",
   "metadata": {},
   "source": [
    "**6. the 'Matcher'**\n",
    "\n",
    "A data structure for rule-based pattern matching in text. Matchers allow you to define patterns of tokens or text entities to extract specific information from text data.\n",
    "    \n",
    "For all matcher methods and attributes:\n",
    "https://spacy.io/api/matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce1fbed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Matcher import\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee9538c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and Configuring a Matcher\n",
    "\n",
    "# Create matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define pattern\n",
    "pattern = [{\"LOWER\":\"example\"}, {\"POS\":\"NOUN\"}]\n",
    "# matches the word 'example' followed by a noun\n",
    "\n",
    "# Add pattern to Matcher with a name\n",
    "pattern_name = \"pattern_V1\"\n",
    "matcher.add(pattern_name, [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe71c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Match Format:\n",
      "(Match_ID, Start_index, End_index)\n",
      "\n",
      "Matches:\n",
      "[(16905033022423864804, 3, 5)]\n",
      "\n",
      "Matched text: 'example match'\n"
     ]
    }
   ],
   "source": [
    "# Using a matcher\n",
    "\n",
    "text = \"This is an example match\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Match doc\n",
    "matches = matcher(doc)\n",
    "print(\n",
    "f\"\"\"\n",
    "Match Format:\n",
    "(Match_ID, Start_index, End_index)\n",
    "\n",
    "Matches:\n",
    "{matches}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Extract info from matches\n",
    "for match_id, start, end in matches:\n",
    "    match = doc[start:end].text\n",
    "    print(f\"Matched text: '{match}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "961d92c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: 'You are awesome'\n",
      "Match: 'your programs are fantastic'\n"
     ]
    }
   ],
   "source": [
    "# More in depth patterns\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Pattern to match positive descriptions in second person\n",
    "pattern = [\n",
    "    {\"LOWER\": {\"IN\": [\"you\", \"your\", \"you're\"]}, \"POS\": \"PRON\"},\n",
    "    # Pattern must start with \"You\" and the part-of-speech must be a pronoun\n",
    "    {\"POS\": {\"IN\": [\"NOUN\", \"VERB\", \"ADJ\", \"AUX\"]}, \"OP\": \"*\"},\n",
    "    # The middle of the pattern\n",
    "    # POS matches tokens with a part-of-speech of either VERB or ADJ\n",
    "    # IN means the attribute (POS) value must be a member of the list provided\n",
    "    # OP being * is an operator that allows 0 or more tokens to match this part\n",
    "    {\"TEXT\": {\"IN\": [\"good\", \"awesome\", \"great\", \"fantastic\", \"amazing\"]}, \"OP\": \"+\"}\n",
    "    # + OP requires 1 or more matches\n",
    "]\n",
    "\n",
    "matcher.add(\"positive_description\", [pattern])\n",
    "\n",
    "text = \"You are awesome and your programs are fantastic!\"\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    match = doc[start_index:end_index]\n",
    "    print(f\"Match: '{match}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55996f5",
   "metadata": {},
   "source": [
    "<u>More on Patterns:<u>\n",
    "\n",
    "Matchers can also use patterns from Regex such as:\n",
    "[{\"TEXT\": {\"REGEX\": r\"^\\d{3}\"}}]\n",
    "\n",
    "\n",
    "Refer to the design document link provided above for more in-depth description including operators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c9252",
   "metadata": {},
   "source": [
    "**SpaCy Key Concepts and Functions**\n",
    "1. Tokenization\n",
    "2. Part-of-Speech Tagging (POS)\n",
    "3. Named Entity Recognition (NER)\n",
    "4. Dependency Parsing\n",
    "5. Lemmatization\n",
    "6. Stop Words\n",
    "7. Text Vectors\n",
    "8. Rule-Based Matching\n",
    "9. Custom Pipelines\n",
    "10. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d4d2d9",
   "metadata": {},
   "source": [
    "**Setup:**\n",
    "\n",
    "Please refer to the top of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b6cdf",
   "metadata": {},
   "source": [
    "**1. Tokenization**\n",
    "\n",
    "Tokenization is the process of splitting text into individual words, phrases, or symbols (tokens). Tokenization can be handled differently based on its approach to contractions and punctuation.\n",
    "\n",
    "<u>Usage:<u> \n",
    "* The first step in NLP as it breaks down text into usable pieces for analysis.\n",
    "    \n",
    "<u>Refer to:<u>\n",
    "* Token data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5796dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: These are all tokens. These are too.\n",
      "\n",
      "Index 0: These\n",
      "Index 1: are\n",
      "Index 2: all\n",
      "Index 3: tokens\n",
      "Index 4: .\n",
      "Index 5: These\n",
      "Index 6: are\n",
      "Index 7: too\n",
      "Index 8: .\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# natural language processor\n",
    "\n",
    "\n",
    "# Process text\n",
    "text = \"These are all tokens. These are too.\"\n",
    "doc = nlp(text)\n",
    "# send text through a NLP pipeline to tokenize the text\n",
    "print(f\"Original Text: {text}\\n\")\n",
    "\n",
    "\n",
    "# The tokens are now stored as a list of Token objects inside of the doc\n",
    "for index, token in enumerate(doc):\n",
    "    print(f\"Index {index}: {token.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c0213",
   "metadata": {},
   "source": [
    "**2. Part-of-Speech Tagging (POS)**\n",
    "\n",
    "POS tagging assigns grammatical parts of speech to tokens. These POS tags represent the syntactic role of a token within its sentence.\n",
    "\n",
    "<u>Usage:<u> \n",
    "* Used for understanding sentence structure and information retrieval.\n",
    "\n",
    "<u>Tags:<u> \n",
    "    \n",
    "For more tags including detailed tags, see: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
    "\n",
    "Tags are an abbreviated representation of a part of speech. When you tag a token, you are determining what part of speech that word is.\n",
    "\n",
    "Here are some examples to help grasp the idea:\n",
    "\n",
    "* NN: noun, common, singular or mass\n",
    "\n",
    "    * Ex words:\n",
    "        * common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
    "        investment slide humour falloff slick wind hyena override subhumanity\n",
    "        machinist\n",
    "\n",
    "\n",
    "* NNP: noun, proper, singular\n",
    "\n",
    "    * Ex words:\n",
    "        * Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
    "        Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
    "        Shannon A.K.C. Meltex Liverpool\n",
    "\n",
    "\n",
    "* NNS: noun, common, plural\n",
    "\n",
    "    * Ex words:\n",
    "        * undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
    "        divestitures storehouses designs clubs fragrances averages\n",
    "        subjectivists apprehensions muses factory-jobs\n",
    "\n",
    "* VB: verb, base form\n",
    "\n",
    "    * Ex words:\n",
    "        * ask assemble assess assign assume atone attention avoid bake balkanize\n",
    "        bank begin behold believe bend benefit bevel beware bless boil bomb\n",
    "        boost brace break bring broil brush build\n",
    "\n",
    "\n",
    "* VBD: verb, past tense\n",
    "\n",
    "    * Ex words:\n",
    "        * dipped pleaded swiped regummed soaked tidied convened halted registered\n",
    "        cushioned exacted snubbed strode aimed adopted belied figgered\n",
    "        speculated wore appreciated contemplated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0551572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: These are tokens with tags.\n",
      "\n",
      "\n",
      "    Token: These\n",
      "    POS tag: PRON\n",
      "    Detailed POS tag: DT\n",
      "\n",
      "    Token: are\n",
      "    POS tag: AUX\n",
      "    Detailed POS tag: VBP\n",
      "\n",
      "    Token: tokens\n",
      "    POS tag: NOUN\n",
      "    Detailed POS tag: NNS\n",
      "\n",
      "    Token: with\n",
      "    POS tag: ADP\n",
      "    Detailed POS tag: IN\n",
      "\n",
      "    Token: tags\n",
      "    POS tag: NOUN\n",
      "    Detailed POS tag: NNS\n",
      "\n",
      "    Token: .\n",
      "    POS tag: PUNCT\n",
      "    Detailed POS tag: .\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process text\n",
    "text = \"These are tokens with tags.\"\n",
    "doc = nlp(text)\n",
    "print(f\"Original Text: {text}\\n\")\n",
    "\n",
    "\n",
    "# Accessing Part-of-Speech Tags\n",
    "for token in doc:\n",
    "    print(f\"\"\"\n",
    "    Token: {token.text}\n",
    "    POS tag: {token.pos_}\n",
    "    Detailed POS tag: {token.tag_}\"\"\")\n",
    "    # .pos_ returns the word form of .pos, which is a number\n",
    "    # tag_ returns a more specific tag for the token in its context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d279ec5",
   "metadata": {},
   "source": [
    "**3. Named Entity Recognition (NER)**\n",
    "\n",
    "NER identifies and categorizes named entities in text. These include names of people, organizations, dates, locations, and even money.\n",
    "\n",
    "<u>Usage:<u>\n",
    "* NER is used for information extraction, entity linking, and data mining.\n",
    "    \n",
    "<u>Entity Labels:<u>\n",
    "* Each named entity is assigned an entity label that describes the type of entity it represents (e.g. PERSON, ORGANIZATION, DATE, GEO-POLITICAL ENTITY, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7acd5872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: SpaCy version 1.0 was released on October 19, 2016 by its main developers; Mathew Honnibal and Ines Montani under their MIT License.\n",
      "\n",
      "Entity: 1.0 \n",
      "Label: CARDINAL\n",
      "\n",
      "Entity: October 19, 2016 \n",
      "Label: DATE\n",
      "\n",
      "Entity: Mathew Honnibal \n",
      "Label: PERSON\n",
      "\n",
      "Entity: Ines Montani \n",
      "Label: PERSON\n",
      "\n",
      "Entity: MIT License \n",
      "Label: ORG\n",
      "\n",
      "\n",
      "Visualization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">SpaCy version \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1.0\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " was released on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    October 19, 2016\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " by its main developers; \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mathew Honnibal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ines Montani\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " under their \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    MIT License\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process text\n",
    "text = \"SpaCy version 1.0 was released on October 19, 2016 by its main developers; Mathew Honnibal and Ines Montani under their MIT License.\"\n",
    "doc = nlp(text)\n",
    "print(f\"Original Text: {text}\\n\")\n",
    "\n",
    "\n",
    "# Accessing Named Entities\n",
    "for entity in doc.ents:\n",
    "    # .ents returns a tuple of every token that is an entitiy\n",
    "    print(f\"Entity: {entity.text} \\nLabel: {entity.label_}\\n\")\n",
    "    # .label_ returns the word form of its entity label\n",
    "\n",
    "    \n",
    "# Visualization\n",
    "from spacy.displacy import render\n",
    "\n",
    "print(\"\\nVisualization:\")\n",
    "render(doc, style=\"ent\")\n",
    "# Highlights each entity within a doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7460c35a",
   "metadata": {},
   "source": [
    "**4. Dependency Parsing**\n",
    "\n",
    "Dependency parsing analyzes the grammatical structure of a sentence by identifying the relationships between words. It uses an arc-tree structure to represent dependencies. (which words refer to which words in a sentence)\n",
    "\n",
    "<u>Usage:<u>\n",
    "* Essential for understandning the relationship between words of a sentence. Helps identify subkects, predicates, and objects in a sentence and is used for translation and question-answering systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b482a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Brandon has a lot of time-consuming homework.\n",
      "\n",
      "\n",
      "    Token: Brandon\n",
      "    Dependency Label: nsubj\n",
      "    Head Token: has\n",
      "\n",
      "    Token: has\n",
      "    Dependency Label: ROOT\n",
      "    Head Token: has\n",
      "\n",
      "    Token: a\n",
      "    Dependency Label: det\n",
      "    Head Token: lot\n",
      "\n",
      "    Token: lot\n",
      "    Dependency Label: dobj\n",
      "    Head Token: has\n",
      "\n",
      "    Token: of\n",
      "    Dependency Label: prep\n",
      "    Head Token: lot\n",
      "\n",
      "    Token: time\n",
      "    Dependency Label: npadvmod\n",
      "    Head Token: consuming\n",
      "\n",
      "    Token: -\n",
      "    Dependency Label: punct\n",
      "    Head Token: consuming\n",
      "\n",
      "    Token: consuming\n",
      "    Dependency Label: amod\n",
      "    Head Token: homework\n",
      "\n",
      "    Token: homework\n",
      "    Dependency Label: pobj\n",
      "    Head Token: of\n",
      "\n",
      "    Token: .\n",
      "    Dependency Label: punct\n",
      "    Head Token: has\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process text\n",
    "text = \"Brandon has a lot of time-consuming homework.\"\n",
    "doc = nlp(text)\n",
    "print(f\"Original Text: {text}\\n\")\n",
    "\n",
    "\n",
    "# Accessing dependency information\n",
    "for token in doc:\n",
    "    print(f\"\"\"\n",
    "    Token: {token.text}\n",
    "    Dependency Label: {token.dep_}\n",
    "    Head Token: {token.head.text}\"\"\")\n",
    "    # A head token is a word that modifies the meaning of another word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e73b27b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Noun Phrases:\n",
      "Brandon\n",
      "a lot\n",
      "time-consuming homework\n"
     ]
    }
   ],
   "source": [
    "# Extract noun phrases\n",
    "# noun phrases are chunks of text that include:\n",
    "    # a noun and the words describing that noun\n",
    "print(\"\\nNoun Phrases:\")\n",
    "for noun_phrase in doc.noun_chunks:\n",
    "    # .noun_chunks returns an object\n",
    "    print(noun_phrase.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e110237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualization:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"ea35a49d85484dcfaa60c1fc5c9a0dae-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Brandon</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">has</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">lot</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">time-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">consuming</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">homework.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-6\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ea35a49d85484dcfaa60c1fc5c9a0dae-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization\n",
    "from spacy.displacy import render\n",
    "\n",
    "print(\"\\nVisualization:\")\n",
    "render(doc, style=\"dep\")\n",
    "# prints an arc-tree representing each word's dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c356723",
   "metadata": {},
   "source": [
    "**5. Lemmatization**\n",
    "\n",
    "Lemmatization reduces words to their base or dictionary form, helping standardize words.\n",
    "\n",
    "<u>Usage:<u>\n",
    "* Normalizing text makes it easier to compare and analyze. This helps in information retrieval and text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ab07d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Programming is a diverse field\n",
      "\n",
      "Lemmatization:\n",
      "\n",
      "    Token: Programming\n",
      "    Lemma: programming\n",
      "\n",
      "    Token: is\n",
      "    Lemma: be\n",
      "\n",
      "    Token: a\n",
      "    Lemma: a\n",
      "\n",
      "    Token: diverse\n",
      "    Lemma: diverse\n",
      "\n",
      "    Token: field\n",
      "    Lemma: field\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process text\n",
    "text = \"Programming is a diverse field\"\n",
    "doc = nlp(text)\n",
    "print(f\"Original Text: {text}\\n\")\n",
    "\n",
    "\n",
    "# find lemmas (Lemmatize)\n",
    "print(\"Lemmatization:\")\n",
    "for token in doc:\n",
    "    print(f\"\"\"\n",
    "    Token: {token.text}\n",
    "    Lemma: {token.lemma_}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b1547",
   "metadata": {},
   "source": [
    "**6. Stop Words**\n",
    "\n",
    "Stop words are common words with little contextual meaning. They are considered noise words.\n",
    "\n",
    "<u>Usage:<u>\n",
    "* Removing stop words improves efficiency and accuracy when classifying text or analyzing sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f242eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: The bunny ran over the turtle in spite.\n",
      "\n",
      "Stop word: The\n",
      "Stop word: over\n",
      "Stop word: the\n",
      "Stop word: in\n",
      "\n",
      "Flitered text: bunny ran turtle spite .\n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process text\n",
    "text = \"The bunny ran over the turtle in spite.\"\n",
    "doc = nlp(text)\n",
    "print(f\"Original Text: {text}\\n\")\n",
    "\n",
    "\n",
    "# Find stop words\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        # .is_stop method returns True if the token is a stop word\n",
    "        print(f\"Stop word: {token.text}\")\n",
    "        \n",
    "        \n",
    "# Recreate text without stop words\n",
    "filtered_text = \" \".join(token.text for token in doc if not token.is_stop)\n",
    "# Concatenates every token's text within a doc that is not a stop word with a space in between\n",
    "print(f\"\\nFlitered text: {filtered_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692e13bb",
   "metadata": {},
   "source": [
    "**7. Text Vectors**\n",
    "\n",
    "Text vectors represent words, phrases, or documents as numerical vectors in multi-dimensional space. These vectors capture semantic information, allowing for comparisons between text elements.\n",
    "\n",
    "<u>Usage:<u>\n",
    "* Finsing document similarity, clustering, and recommendation systems. Vectors enable computation of similarities between texts based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40f1b4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I ate an apple.\n",
      "\n",
      "Word Vectors:\n",
      "\n",
      "    Token: I\n",
      "    Vector shape: 300 value long\n",
      "    First 4 vector values: \n",
      "    [-1.8607   0.15804 -4.1425  -8.6359 ]\n",
      "\n",
      "    Token: ate\n",
      "    Vector shape: 300 value long\n",
      "    First 4 vector values: \n",
      "    [ 3.3417  -8.3958   0.35492  1.8888 ]\n",
      "\n",
      "    Token: an\n",
      "    Vector shape: 300 value long\n",
      "    First 4 vector values: \n",
      "    [11.492   2.9806 15.917  -1.1007]\n",
      "\n",
      "    Token: apple\n",
      "    Vector shape: 300 value long\n",
      "    First 4 vector values: \n",
      "    [-1.0084  -2.0308  -0.64185  2.6928 ]\n",
      "\n",
      "    Token: .\n",
      "    Vector shape: 300 value long\n",
      "    First 4 vector values: \n",
      "    [-0.076454 -4.6896   -4.0431   -3.4333  ]\n",
      "\n",
      "Document info:\n",
      "\n",
      "    Document Vector Shape: 300 values long\n",
      "    First 4 vector values:\n",
      "    [-0.076454 -4.6896   -4.0431   -3.4333  ]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process text\n",
    "text = \"I ate an apple.\"\n",
    "doc = nlp(text)\n",
    "print(f\"Original Text: {text}\\n\")\n",
    "\n",
    "\n",
    "# Find word vectors\n",
    "print(\"Word Vectors:\")\n",
    "for token in doc:\n",
    "    print(f\"\"\"\n",
    "    Token: {token.text}\n",
    "    Vector shape: {token.vector.shape[0]} value long\n",
    "    First 4 vector values: \n",
    "    {token.vector[:4]}\"\"\")\n",
    "# .vector returns a long list of vector values\n",
    "# .vector.shape returns a tuple of the dimensions of the vector\n",
    "\n",
    "\n",
    "# Find vector of entire doc\n",
    "print(\"\\nDocument info:\")\n",
    "print(f\"\"\"\n",
    "    Document Vector Shape: {doc.vector.shape[0]} values long\n",
    "    First 4 vector values:\n",
    "    {token.vector[:4]}\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "25985854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two documents: \n",
      "0.7007695012580235\n",
      "(Similar)\n"
     ]
    }
   ],
   "source": [
    "# Find similarity between tokens or documents\n",
    "\n",
    "# Create doc to compare with\n",
    "comparison_text = \"I ate a banana today\"\n",
    "comparison_doc = nlp(comparison_text)\n",
    "\n",
    "# Find similarity\n",
    "similarity = doc.similarity(comparison_doc)\n",
    "# Returns a \n",
    "print(f\"Similarity between the two documents: \\n{similarity}\")\n",
    "\n",
    "if similarity == 1:\n",
    "    phrase = \"(Exact Same)\"\n",
    "elif similarity >= .75:\n",
    "    phrase = \"(Very Similar)\"\n",
    "elif similarity >= .5:\n",
    "    phrase = \"(Similar)\"\n",
    "elif similarity >= .25:\n",
    "    phrase = \"(Somewhat Similar)\"\n",
    "else:\n",
    "    phrase = \"(Note Similar)\"\n",
    "\n",
    "print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed8fa0",
   "metadata": {},
   "source": [
    "**8. Rule-Based Matching**\n",
    "\n",
    "Rule-based matching allows for defined patterns of tokens using specified rules and extract information based on these patterns. SpaCy has an implemented rule-based matcher.\n",
    "\n",
    "<u>Usage:<u>\n",
    "* Used for extracting specific information or entities from text, even when they don't fit standard NER categories. Allows for the creation of custom entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84b57386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55407e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Deine a pattern (explained in more detail later)\n",
    "pattern = [{\"LOWER\": \"example\"}]\n",
    "\n",
    "# Add pattern to matcher\n",
    "matcher.add(\"ExamplePattern\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fe1934a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text and apply matcher\n",
    "\n",
    "text = \"Here is example 1 and example 2.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d428852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matches are tuples in this format:\n",
      "(Match_ID, Start_index, End index)\n",
      "\n",
      "For Example:\n",
      "(583384548970338471, 2, 3)\n",
      "\n",
      "Matches:\n",
      "\n",
      "    Matched ID: 583384548970338471\n",
      "    Start and End Index: (2, 3)\n",
      "    Match Text: example\n",
      "    \n",
      "\n",
      "    Matched ID: 583384548970338471\n",
      "    Start and End Index: (5, 6)\n",
      "    Match Text: example\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Access matches\n",
    "\n",
    "print(f\"\"\"\n",
    "Matches are tuples in this format:\n",
    "(Match_ID, Start_index, End index)\n",
    "\n",
    "For Example:\n",
    "{matches[0]}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Matches:\")\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    match_text = doc[start:end].text\n",
    "    print(f\"\"\"\n",
    "    Matched ID: {match_id}\n",
    "    Start and End Index: ({start}, {end})\n",
    "    Match Text: {match_text}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d77c73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing patterns\n",
    "\n",
    "matcher.remove(\"ExamplePattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd720ab",
   "metadata": {},
   "source": [
    "<u>Defining Patterns<u>\n",
    "* Patterns are defined using dictionaries that specify the criteria for matching.\n",
    "\n",
    "<u>Common Keys<u>\n",
    "* \"LOWER\": Matches the lowercase form of the token's text.\n",
    "* \"TEXT\": Matches the exact text of the token.\n",
    "* \"LEMMA\": Matches the lemma of the token.\n",
    "* \"POS\": Matches the part-of-speech tag of the token.\n",
    "* \"TAG\": Matches the fine-grained part-of-speech tag of the token.\n",
    "* \"ORTH\": Matches the exact text, including case.\n",
    "* \"ENT_TYPE\": Matches the entity type of the token (for named entity recognition).\n",
    "* \"IS_ALPHA\": Matches if the token is an alphabetic character.\n",
    "* \"IS_DIGIT\": Matches if the token is a digit.\n",
    "    \n",
    "<u>Modifer Keys<u>\n",
    "* \"IN\": \n",
    "    * Matches a token's text against a list of possible values\n",
    "    * Means the attribute (key) before it must be a member of the list provided\n",
    "* \"OP\"\n",
    "    * Specifies the repitition of a token in the pattern\n",
    "    * Allows you to define the recurrance of a matching token using these operators:\n",
    "        * \"*\" (Zero or more)\n",
    "        * \"+\" (One or more)\n",
    "        * \"?\" (Zero or one)\n",
    "        * \"{n}\" (Exactly n times)\n",
    "        * \"{n,}\" (n or more times)\n",
    "        * \"{n,m}\" (Between n and m times)\n",
    "    \n",
    "<u>Using Regex<u>\n",
    "* Regex can be used with the \"REGEX\" key to match the \"TEXT\" key. For example:\n",
    "    * [{\"TEXT\": {\"REGEX\": r\"^\\d{3}\"}}]\n",
    "    * Will match any text that matches that regex (3 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df9f0474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: 'You are awesome'\n",
      "Match: 'your programs are fantastic'\n"
     ]
    }
   ],
   "source": [
    "# Here is an example from earlier\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Pattern to match positive descriptions in second person\n",
    "pattern = [\n",
    "    {\"LOWER\": {\"IN\": [\"you\", \"your\", \"you're\"]}, \"POS\": \"PRON\"},\n",
    "    # Pattern must start with \"You\" and the part-of-speech must be a pronoun\n",
    "    {\"POS\": {\"IN\": [\"NOUN\", \"VERB\", \"ADJ\", \"AUX\"]}, \"OP\": \"*\"},\n",
    "    # The middle of the pattern\n",
    "    # POS matches tokens with a part-of-speech of either VERB or ADJ\n",
    "    # IN means the attribute (POS) value must be a member of the list provided\n",
    "    # OP being * is an operator that allows 0 or more tokens to match this part\n",
    "    {\"TEXT\": {\"IN\": [\"good\", \"awesome\", \"great\", \"fantastic\", \"amazing\"]}, \"OP\": \"+\"}\n",
    "    # + OP requires 1 or more matches\n",
    "]\n",
    "\n",
    "matcher.add(\"positive_description\", [pattern])\n",
    "\n",
    "text = \"You are awesome and your programs are fantastic!\"\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start_index, end_index in matches:\n",
    "    match = doc[start_index:end_index]\n",
    "    print(f\"Match: '{match}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e3834",
   "metadata": {},
   "source": [
    "**9. Custom Pipelines**\n",
    "\n",
    "Custom pipelines allow you to create processing steps tailored to your specific NLP tasks. You can add custom components or functions to process text.\n",
    "\n",
    "<u>Usage:<u>\n",
    "* Handy when you need a unique preprocessing or analysis requirement that aren't covered in the default spaCy pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa6e8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a blank english model\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "# a blank model has no pre-defined components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef5431e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom pipeline components\n",
    "\n",
    "# Add spacy decorator to define a custom component with a specified name\n",
    "\n",
    "@spacy.Language.component(\"reverse_doc\")\n",
    "def custom_component1(doc):\n",
    "    # Define custom process (reverse token order)\n",
    "    reversed_tokens = list(reversed(doc))\n",
    "    reversed_text = ' '.join(token.text for token in reversed_tokens)\n",
    "    \n",
    "    # Return a doc\n",
    "    reversed_doc = nlp.make_doc(reversed_text)\n",
    "    \n",
    "    return reversed_doc\n",
    "\n",
    "\n",
    "# Defining custom attributes inside a component \n",
    "\n",
    "@spacy.Language.component(\"token_counter\")\n",
    "def custom_component2(doc):\n",
    "    # Define custom process (count reversed tokens)\n",
    "    # Define custom attribute\n",
    "    \n",
    "    if (len(doc) % 2) == 0:\n",
    "        doc._.reversed_tokens = len(doc)\n",
    "    else:\n",
    "        doc._.reversed_tokens = len(doc) - 1\n",
    "    # custom doc attributes start with 'doc._.'\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7b8706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom components to the pipeline\n",
    "\n",
    "nlp.add_pipe(\"reverse_doc\", last=True)\n",
    "# Adds first component to the end of the default pipeline\n",
    "nlp.add_pipe(\"token_counter\", last=True)\n",
    "# component1 was added first, so it will perform before component2\n",
    "\n",
    "# Register custom attribute\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"reversed_tokens\", default=None, force=True)\n",
    "# Doc object is given a new attribute named 'reversed_tokens'\n",
    "# default value = None\n",
    "# force = True allows the extension to be overwritten\n",
    "\n",
    "\n",
    "# Adding duplicate components will result in an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f0c51c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: This is an example text.\n",
      "\n",
      "Post-processing: \n",
      "Index 0: .\n",
      "Index 1: text\n",
      "Index 2: example\n",
      "Index 3: an\n",
      "Index 4: is\n",
      "Index 5: This\n",
      "\n",
      "Tokens reverse: 6\n"
     ]
    }
   ],
   "source": [
    "# Process text with the custom pipeline\n",
    "\n",
    "text = \"This is an example text.\"\n",
    "doc = nlp(text)\n",
    "# The doc object will represent the text after being processed\n",
    "# by the custom components\n",
    "\n",
    "print(f\"Original text: {text}\\n\")\n",
    "\n",
    "print(\"Post-processing: \")\n",
    "for i, token in enumerate(doc):\n",
    "    print(f\"Index {i}: {token}\")\n",
    "    \n",
    "print(f\"\\nTokens reverse: {doc._.reversed_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e974e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('token_counter', <function __main__.custom_component2(doc)>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing custom components\n",
    "nlp.remove_pipe(\"reverse_doc\")\n",
    "nlp.remove_pipe(\"token_counter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b3509d",
   "metadata": {},
   "source": [
    "**10. Text Classification**\n",
    "\n",
    "Text classification assigns predefined categories or labels to text documents.\n",
    "\n",
    "<u>Usage:<u>\n",
    "* Valuable for automating tasks such as sorting documents or emails into folders, analyzing review, or categorizing articles.\n",
    "    \n",
    "<u>Training Info:<u>\n",
    "* The Example object is used: https://spacy.io/api/example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa44f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "# You can also start with a pretrained model as well (e.g. \"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5955472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining \n",
    "\n",
    "# Define text classification model\n",
    "\n",
    "textcat = nlp.add_pipe(\"textcat\")\n",
    "# Adding a copy results in error (remove copy and try again)\n",
    "# text categorizer requires the name 'textcat'\n",
    "# 'textcat' is a predefined spacy factory (class) component \n",
    "\n",
    "\n",
    "# Define labels\n",
    "# These are categorizers that the model can predict\n",
    "textcat.add_label(\"English\")\n",
    "textcat.add_label(\"Code\")\n",
    "\n",
    "\n",
    "# Define training data\n",
    "\n",
    "training_data = [\n",
    "    (\"I've had a great day today.\", {\"cats\": {\"English\": 1, \"Code\": 0}}),\n",
    "    (\"for data in data_dict:\", {\"cats\": {\"English\": 0, \"Code\": 1}}),\n",
    "    (\"var = (x+1)\", {\"cats\": {\"English\": 0, \"Code\": 1}}),\n",
    "    (\"nlp = spacy.load('en_core_web_md')\", {\"cats\": {\"English\": 0, \"Code\": 1}}),\n",
    "    (\"Watch out! There's a tumbling duck!\", {\"cats\": {\"English\": 1, \"Code\": 0}}),\n",
    "    (\"Please read the document.\", {\"cats\": {\"English\": 0, \"Code\": 1}})\n",
    "]\n",
    "# All data samples should be a tuple with a string and a dictionary \n",
    "# Dictionaries represent the labels/categories by having the \"cats\" key\n",
    "# Categories are predefined based off of the training string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97bf9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train text classification mode\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "\n",
    "n_training = 10\n",
    "# number of training iterations\n",
    "\n",
    "# Define optimizer used for training\n",
    "optimizer = nlp.initialize()\n",
    "# An optimizer is a component that can adjust the parameters of a model\n",
    "# (e.g. textcat) during training to minimize loss in accuracy\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_training):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(training_data)\n",
    "    examples = []\n",
    "    \n",
    "    # Make a collection of examples\n",
    "    for text, annotations in training_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        # Another way to make a doc\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        # Example object encases the doc and its annotation in a format suitable \n",
    "        # for training models\n",
    "        examples.append(example)\n",
    "        # Creates example from dictionary using Example object's method 'from_dict'\n",
    "        \n",
    "    # Update model with training data \n",
    "    losses = textcat.update(examples, sgd=optimizer, drop=0.5)\n",
    "    # 'update' method takes a list of Example objects (or 1) and updates the model\n",
    "    # optimizer reduces loss\n",
    "    # drop is a proportion of neurons in a neural network being unused to increase generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdc9676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predictions: \n",
      "{'English': 0.4103195369243622, 'Code': 0.5896804928779602}\n",
      "(More code than english)\n"
     ]
    }
   ],
   "source": [
    "# Classify text\n",
    "\n",
    "test_text = \"doc = nlp(text_rep)\"\n",
    "doc = nlp(test_text)\n",
    "\n",
    "prediction = doc.cats\n",
    "# gets the predicted labels (categories) and their probabilities\n",
    "\n",
    "print(f\"Label Predictions: \\n{prediction}\")\n",
    "\n",
    "Code = prediction['Code']\n",
    "English = prediction['English']\n",
    "\n",
    "if Code > English:\n",
    "    phrase = \"(More code than english)\"\n",
    "elif Code == Enlish:\n",
    "    phrase = \"(Equal parts code to english)\"\n",
    "else:\n",
    "    phrase = \"(More english than code)\"\n",
    "    \n",
    "print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e5994073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('textcat', <spacy.pipeline.textcat.TextCategorizer at 0x7f0b5af20340>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing a text classifier\n",
    "nlp.remove_pipe(\"textcat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
